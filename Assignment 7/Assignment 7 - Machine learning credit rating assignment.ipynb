{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;\">Credit rating assignment</h1>\n",
    "<p></p>\n",
    "In this assignment, we'll work our way through a simple ML exercise. Machine learning is an iterative process that starts with feature engineering (making the features ready for ML), works it way through various models and hyperparameter tuning exercises, until we find a model that seems to work well for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">The problem: Rating creditworthiness of loan applicants</h3>\n",
    "\n",
    "When banks issue loans to individuals, they have two goals that conflict with each other:\n",
    "<ol>\n",
    "    <li>Give as many loans as possible (fees, interest, all add to revenue)</li>\n",
    "    <li>Try not to give loans to individuals who won't pay it back (lose money on the loan, collection costs, etc.)</li>\n",
    "</ol>\n",
    "    \n",
    "<li>A typical machine learning program in this space tries to find a suitable tradeoff between finding many good loans and not calling a bad loan good</li>\n",
    "\n",
    "<li>In this assignment, we'll try to build a \"good\" model that finds a good tradeoff between these two objectives</li>\n",
    "\n",
    "<li>In machine learning terms, the proportion of times we get our guess right (i.e., we call a bad loan a bad loan and a good loan a good loan divided by the total number of cases) is called <span style=\"color:blue\">accuracy</span></li>\n",
    "\n",
    "<li>The proportion of actual good loans that we identify as good loans is known as <span style=\"color:blue\">recall</span></li>\n",
    "\n",
    "<li>The probability that if a loan is called good it actually is good is called <span style=\"color:blue\">precision</span></li>\n",
    "\n",
    "<li>The precision recall tradeoff is measured through a score called <span style=\"color:blue\">f1 score</span></li>\n",
    "\n",
    "<li>An important part of running an ML model is trying to figure out \"which metric is right for you\"</li>\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "<ol>\n",
    "    <li>We'll try the SGD classifier, tune hyperparameters using grid search, and examine the results</li>\n",
    "    <li>then, set up the data for a random forest classifier, run a grid search, and examine the results</li>\n",
    "        <ul><li>finally, run a couple of gradient booster models</li></ul>\n",
    "    <li>draw precision recall curves and roc curves for the two classifiers and compare the results</li>\n",
    "    <li>note that grid search is a computing intensive activity. I've simplified the search to a few options but even those can take a long while (less than 15 minutes on my laptop but could be a couple of hours if you have an older machine)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">The models</h3>\n",
    "<p></p>\n",
    "<li><b>Model 1 SGD Classifier</b>: Vanilla version with max_iter set to 1000</li>\n",
    "<li><b>Model 2 SGD Classifier round 2</b>: SGD Classifier with positive cases assigned a higher weight. One issue with our data is that positive cases are vastly outnumbered by negative cases (in other words, a model that says all cases are negative will have a pretty good accuracy). By overweighting positive cases in our model, we increase the efficacy of the model in finding an actual good solution</li>\n",
    "<li><b>Model 3 SGD Classifier round 3</b>: Best SGD Classifier model after grid search</li>\n",
    "<li><b>Model 4 Random Forest Classifier round 1</b>: Random Forest Classifier with base parameters (see below)</li>\n",
    "<li><b>Model 5 Random Forest Classifier round 2</b>: Best model from grid search</li>\n",
    "<li><b>Model 6 Gradient Booster Classfier</b></li>\n",
    "<li><b>Model 7 Gradient Booster Classifier (2nd model)</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, collect model metrics in the following dataframe results_df. After each model run, replace the 0.0 with the appropriate metric value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  precision  recall  f1_score  AUC   AP\n",
       "Model                                                 \n",
       "1           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "2           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "3           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "4           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "5           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "6           0.0        0.0     0.0       0.0  0.0  0.0\n",
       "7           0.0        0.0     0.0       0.0  0.0  0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "results_df = pd.DataFrame(np.zeros(shape=(7,6)))\n",
    "results_df.index=[1,2,3,4,5,6,7]\n",
    "results_df.columns = [\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"AUC\",\"AP\"]\n",
    "results_df.index.rename(\"Model\",inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">The data</h3>\n",
    "<p></p>\n",
    "<li>A curated extract from the popular Lending club loan data. The data is in the file loan_data_small.csv</li>\n",
    "<li>The dataset contains information about loan applications. Very basic information about the applicant and the status of the loan</li>\n",
    "<li>The goal of the ML exercise is to build a model that uses information about the loan to predict whether a loan is a \"good\" one (i.e., it will be paid back) or a \"bad\" one (the money is unrecoverable)</li>\n",
    "<li>Note that we're only using a fraction of the data. If you're interested, I can share the curated extract on a larger fraction which gives better results (but can crash your machine!)</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Data preparation and feature engineering</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Build a binary target</h3>\n",
    "\n",
    "<li>For the purposes of this analysis, drop rows that contain any NaN values</li>\n",
    "<li><b>Target</b>: For the classifier, classify any loans that have a loan_status value of \"Charged Off\",\"Default\", or \"Does not meet the credit policy. Status:Charged Off\" as a bad loan and give these loans a target value of 1 (we're predicting bad loans)</li>\n",
    "<li><b>Input features</b>: create the input feature dataframe (i.e., drop any columns that are not an independent variable). The input variables we're interested in are \"int_rate\", \"grade\", \"home_ownership\",\"annual_income\", \"loan_amt\", and \"purpose\"</li>\n",
    "<p></p>\n",
    "<li>The data should look like:</li>\n",
    "<pre>\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 565167 entries, 0 to 565166\n",
    "Data columns (total 7 columns):\n",
    " #   Column          Non-Null Count   Dtype  \n",
    "---  ------          --------------   -----  \n",
    " 0   Unnamed: 0.1    565167 non-null  int64  \n",
    " 1   int_rate        565167 non-null  float64\n",
    " 2   grade           565167 non-null  object \n",
    " 3   home_ownership  565167 non-null  object \n",
    " 4   annual_inc      565167 non-null  float64\n",
    " 5   loan_amnt       565167 non-null  int64  \n",
    " 6   purpose         565167 non-null  object \n",
    "dtypes: float64(2), int64(2), object(3)\n",
    "memory usage: 30.2+ MB\n",
    "Out[108]:\n",
    "0         False\n",
    "1          True\n",
    "2         False\n",
    "3         False\n",
    "4          True\n",
    "          ...  \n",
    "565162    False\n",
    "565163    False\n",
    "565164    False\n",
    "565165     True\n",
    "565166    False\n",
    "Name: loan_status, Length: 565167, dtype: bool\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 565167 entries, 0 to 565166\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Unnamed: 0.1    565167 non-null  int64  \n",
      " 1   int_rate        565167 non-null  float64\n",
      " 2   grade           565167 non-null  object \n",
      " 3   home_ownership  565167 non-null  object \n",
      " 4   annual_inc      565167 non-null  float64\n",
      " 5   loan_amnt       565167 non-null  int64  \n",
      " 6   purpose         565167 non-null  object \n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 30.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#read the file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"Resources/loan_data_small.csv\")\n",
    "\n",
    "#Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Prepare the y (target) variable\n",
    "#The target variable should be 1 if loan_status is \"Charged Off\",\"Default\", or \"Does not meet the credit policy. Status:Charged Off\"\n",
    "#And 0 otherwise\n",
    "#(Hint: Create a boolean mask series)\n",
    "\n",
    "y = df[\"loan_status\"].isin([\"Charged Off\",\"Default\",\"Does not meet the credit policy. Status:Charged Off\"])\n",
    "\n",
    "#remove unwanted input features \"Unnamed: 0\" and \"loan_status\"\n",
    "df.drop(columns=[\"Unnamed: 0\",\"loan_status\"],inplace=True)\n",
    "\n",
    "#Examine the df and the target\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         False\n",
       "1          True\n",
       "2         False\n",
       "3         False\n",
       "4          True\n",
       "          ...  \n",
       "565162    False\n",
       "565163    False\n",
       "565164    False\n",
       "565165     True\n",
       "565166    False\n",
       "Name: loan_status, Length: 565167, dtype: bool"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Label Encoding</h3>\n",
    "<li>Since we're using regression as our underlying algorithm, all values need to be numerical. ML Models generally deal with numerical data</li>\n",
    "<li>But, <span style=\"color:blue\">grade</span>, <span style=\"color:blue\">purpose</span>, and <span style=\"color:blue\">home_ownership</span> are not</li>\n",
    "</li>\n",
    "<li>sklearn's <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\">LabelEncoder</a> assigns numerical values to categorical data</li>\n",
    "<li>LabelEncoder replaces each categorical string value with an integer - 0, 1, 2, ...</li>\n",
    "<li>After label encoding, df.info() should return:</li>\n",
    "<pre>\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 565167 entries, 0 to 565166\n",
    "Data columns (total 7 columns):\n",
    " #   Column          Non-Null Count   Dtype  \n",
    "---  ------          --------------   -----  \n",
    " 0   Unnamed: 0.1    565167 non-null  int64  \n",
    " 1   int_rate        565167 non-null  float64\n",
    " 2   grade           565167 non-null  int64  \n",
    " 3   home_ownership  565167 non-null  int64  \n",
    " 4   annual_inc      565167 non-null  float64\n",
    " 5   loan_amnt       565167 non-null  int64  \n",
    " 6   purpose         565167 non-null  int64  \n",
    "dtypes: float64(2), int64(5)\n",
    "memory usage: 30.2 MB\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace grade, purpose, and home_ownership by label encoded versions\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df[\"grade\"] = le.fit_transform(df[\"grade\"])\n",
    "df[\"purpose\"] = le.fit_transform(df[\"purpose\"])\n",
    "df[\"home_ownership\"] = le.fit_transform(df[\"home_ownership\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 565167 entries, 0 to 565166\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   Unnamed: 0.1    565167 non-null  int64  \n",
      " 1   int_rate        565167 non-null  float64\n",
      " 2   grade           565167 non-null  int32  \n",
      " 3   home_ownership  565167 non-null  int32  \n",
      " 4   annual_inc      565167 non-null  float64\n",
      " 5   loan_amnt       565167 non-null  int64  \n",
      " 6   purpose         565167 non-null  int32  \n",
      "dtypes: float64(2), int32(3), int64(2)\n",
      "memory usage: 23.7 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">One-hot encoding</h3>\n",
    "\n",
    "<p></p>\n",
    "<li>In regression, the assumption is that values associated with a feature are ordered</li>\n",
    "<li>But, this is not necessarily so for the label encoded categorical values</li>\n",
    "<li>The way to deal with this in regression is to create dummy variables, one for each category, that take the value 1 if the category is present in the row and 0 otherwise</li>\n",
    "<li>In ML, a procedure known as <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot encoding</a> is used to do this conversion</li>\n",
    "<li>One hot encoding is the process of converting a single column of categorical (integer) data with k categories into k-1 columns of 0 or 1 values</li>\n",
    "<li>for example, the array with three possible categories [1,2,3,2,1] will be converted into the matrix:</li>\n",
    "\n",
    "$$\\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "<li>1's are replaced by (0, 0); 2's by (1, 0); and 3's by (0, 1). Note that category 1 is implicitly coded</li>\n",
    "<li><b>Documentation</b>: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Scaling</h3>\n",
    "\n",
    "<p></p>\n",
    "<li>Non-categorical independent variables need to be scaled so that they follow the same underlying distribution</li>\n",
    "<li>We will normalize them so that the mean is 0 and standard deviation is 1 using sklearn's StandardScaler feature transformer</li>\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>All feature transformations can be encapsulated in the sklearn <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html\">make_column_transformer</a> object</li>\n",
    "<li>Use <span style=\"color:blue\">make_column_transformer</span> to encapsulate both the one-hot coding as well as standard scaling. Note that the one-hot encoded columns are not scaled!</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565167, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "#Make a column transformer object that scales (using StandardScaler) the two non-categorical columns\n",
    "# and one hot encodes (using OneHotEncoder) the three categorical columns\n",
    "# Using make_column_transformer \n",
    "preprocess = make_column_transformer(\n",
    "    (StandardScaler(),['int_rate', 'annual_inc'], ),\n",
    "    (OneHotEncoder(categories=\"auto\",drop=\"first\"),['grade', 'home_ownership','purpose'], )\n",
    ")\n",
    "\n",
    "#Generate the independent variable df\n",
    "X = preprocess.fit_transform(df).toarray()\n",
    "X.shape\n",
    "#Should return (565167, 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Train/Test split</h3>\n",
    "\n",
    "<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a></li>\n",
    "<li>split the data into 70% training and 30% testing</li>\n",
    "<li>make sure the x and y datasets are aligned</li>\n",
    "<li>use random_state=42 to get the same split as in my code </li>\n",
    "<li>x and y training data shapes: (395616, 26) (395616,)</li>\n",
    "<li>x and y testing data shapes: (169551, 26) (169551,)</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395616, 26) (395616,)\n",
      "(169551, 26) (169551,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nShould return:\\n(395616, 26) (395616,)\\n(169551, 26) (169551,)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Get x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "#And check the shape\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "\n",
    "\"\"\"\n",
    "Should return:\n",
    "(395616, 26) (395616,)\n",
    "(169551, 26) (169551,)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27542382, -0.54678873,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.81078196,  1.05693436,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.20043776,  0.4221273 ,  1.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.79059571, -0.3930986 ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.18940226, -0.85299293,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.09698959, -0.0656718 ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green\">The models</h1>\n",
    "<li>For each model, do the following</li>\n",
    "<ol>\n",
    "    <li>Fit a classifier to the training data</li>\n",
    "    <li>calculate the metrics</li>\n",
    "    <ul>\n",
    "        <li>training accuracy</li>\n",
    "        <li>testing accuracy</li>\n",
    "        <li>precision on test dataset</li>\n",
    "        <li>recall on test dataset</li>\n",
    "        <li>f1 score on test dataset</li>\n",
    "        <li>area under the curve on test dataset</li>\n",
    "        <li>average precision on the test dataset</li>\n",
    "    </ul>\n",
    "    <li>Write up a brief (pointwise) interpretation of the results\n",
    "</ol>\n",
    "<li>Chart the various metrics</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 1</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Build the model on the training data set</h3>\n",
    "\n",
    "<li>set random_state to 42 (if you want to get the same results that I got) and max_iter to 1000</li>\n",
    "<li>set the loss function to \"log_loss\" (\"log\" if using sklearn 1.0.x or on colab)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8846634109843889\n",
      "0.8843828700508991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou should get:\\n0.8846634109843889\\n0.8843828700508991\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model_1 = SGDClassifier(loss=\"log_loss\",random_state=42, max_iter=1000)\n",
    "model_1.fit(x_train,y_train) #change if you used different variable names\n",
    "\n",
    "print(model_1.score(x_train,y_train))\n",
    "print(model_1.score(x_test,y_test))\n",
    "\"\"\"\n",
    "You should get:\n",
    "0.8846634109843889\n",
    "0.8843828700508991\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 style=\"color:green;\">Model 1 metrics</h3>\n",
    "<li>Report the following on the <b>test</b> data:</li>\n",
    "<ul>\n",
    "<li>the confusion matrix</li>\n",
    "<li>the accuracy, precision, recall, f1-score, AUC, and AP </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[149948      1]\n",
      " [ 19602      0]]\n",
      "Training accuracy:  0.8846634109843889\n",
      "Testing  accuracy:  0.8843828700508991\n",
      "Precision:  0.0\n",
      "Recall:  0.0\n",
      "F1-Score:  0.0\n",
      "AUC:  0.6929621775583543\n",
      "Average Precision:  0.2277691812634675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nYou should see:\\n\\nConfusion Matrix: \\n [[149948      1]\\n [ 19602      0]]\\nTraining accuracy:  0.8846634109843889\\nTesting  accuracy:  0.8843828700508991\\nPrecision:  0.0\\nRecall:  0.0\\nF1-Score:  0.0\\nAUC:  0.692962177388246\\nAverage Precision:  0.11561123201868465\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "from sklearn.metrics import average_precision_score,roc_auc_score\n",
    "\n",
    "cfm = confusion_matrix(y_test,model_1.predict(x_test))\n",
    "accuracy_training = model_1.score(x_train,y_train)\n",
    "accuracy_testing = model_1.score(x_test,y_test)\n",
    "precision = precision_score(y_test,model_1.predict(x_test))\n",
    "recall = recall_score(y_test,model_1.predict(x_test))\n",
    "f1 = f1_score(y_test,model_1.predict(x_test))\n",
    "auc = roc_auc_score(y_test,model_1.predict_proba(x_test)[:,1])\n",
    "ap = average_precision_score(y_test,model_1.predict_proba(x_test)[:,1])\n",
    "\n",
    "print(\"Confusion Matrix: \\n\",cfm)\n",
    "print(\"Training accuracy: \",accuracy_training)\n",
    "print(\"Testing  accuracy: \",accuracy_testing)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1-Score: \",f1)\n",
    "print(\"AUC: \",auc)\n",
    "print(\"Average Precision: \",ap)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "You should see:\n",
    "\n",
    "Confusion Matrix: \n",
    " [[149948      1]\n",
    " [ 19602      0]]\n",
    "Training accuracy:  0.8846634109843889\n",
    "Testing  accuracy:  0.8843828700508991\n",
    "Precision:  0.0\n",
    "Recall:  0.0\n",
    "F1-Score:  0.0\n",
    "AUC:  0.692962177388246\n",
    "Average Precision:  0.11561123201868465\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpret the results</h3>\n",
    "<li>In a few bullet points, write your interpreation of the results. Why are we seeing what we are seeing? Is it useful? Why is the AUC not 0.5?</li>\n",
    "\n",
    "<h4>Interpretation</h4>\n",
    "<li> The model is heavily biased toward the negative class (good loans). The bias is likely because of the class imblanace in the dataset. </li>\n",
    "<li> The training and testing accuracy are approximately 0.884, i.e. the model gets its guess correct 88.4\\% of the time. However, this high accuracy might be misleading because it is always predict loans to be good, regardless of their actual status. i.e. Any model that always predicts the majority class would also achieve a similar accuracy. </li>\n",
    "<li> The precision for this model is 0, which indicates that if model 1 were to label a loan as bad, the probability that it is actually bad is zero. i.e. the model's ability to correct label bad loans when it attempts to is zero.</li>\n",
    "<li> The recall for this model is 0, which indicates the proportion of actual bad loans that our model identifies as bad loans is zero. i.e. the model fails to capture any of the bad loans. </li>\n",
    "<li> The F1 score of 0 suggests that our model is failing in both precision and recall for the positive class (bad loans). </li>\n",
    "<li> The AUC has a value 0.693 (greater than 0.5) indicates that the model has indeed some ability to distinguish between good and bad loans, not merely random guessing. Because our model is heavily biased toward predicting good loans, then this will not produce a 0.5 AUC. </li>\n",
    "<li> The average precision indicates that over different recall value, the model's precision is relatively low. </li>\n",
    "<li> Model 1 achieves the goal to give as many loan as possible, but it is fails to achieve the goal to try not give loans to individuals who won't pay back, i.e. identify bad loans.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results_df</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.884383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.692962</td>\n",
       "      <td>0.227769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  precision  recall  f1_score       AUC        AP\n",
       "Model                                                           \n",
       "1      0.884383        0.0     0.0       0.0  0.692962  0.227769\n",
       "2      0.000000        0.0     0.0       0.0  0.000000  0.000000\n",
       "3      0.000000        0.0     0.0       0.0  0.000000  0.000000\n",
       "4      0.000000        0.0     0.0       0.0  0.000000  0.000000\n",
       "5      0.000000        0.0     0.0       0.0  0.000000  0.000000\n",
       "6      0.000000        0.0     0.0       0.0  0.000000  0.000000\n",
       "7      0.000000        0.0     0.0       0.0  0.000000  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[1] = [accuracy_testing,precision,recall,f1,auc,ap]\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 2</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<li>sklearn's ML models can be given a <span style=\"color:blue\">class_weight</span> parameter</li>\n",
    "<li>weights can be given explicitly or implicitly</li>\n",
    "<li>note that by increasing the weight of the true cases, our model is more likely to find true positives</li>\n",
    "<li>and by decreasing the weight of the true cases, our model is more likely to find true negatives</li>\n",
    "<li>In Model 2, increase the weight of positives by a factor of 9 to balance the positives and negatives</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green\">Build model 2 and report metrics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpret the results</h3>\n",
    "\n",
    "\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results_df</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Tune hyperparameters using grid search</h3>\n",
    "<li><span style=\"color:blue\">parameters</span> versus <span style=\"color:blue\">hyperparameters</span></li>\n",
    "<ul>\n",
    "    <li><span style=\"color:blue\">parameters</span>: the parameters that are necessary for the model to make predictions. For example, the coefficients of the linear equation estimated by the SGD classifier are parameters of the model. Parameters are estimated by the algorithm and from the data</li>\n",
    "    <li><span style=\"color:blue\">hyperparameters</span>: parameters that are external to the model and cannot be estimated from the data. For example, in an SGD classifier, parameters like the loss function, the regularization parameter, stopping rules, etc. are hyper parameters</li>\n",
    "    </ul>\n",
    "<li>In ML, hyperparameters are often set intuitively and then <span style=\"color:red\">tuned</span> using a grid search</li>\n",
    "<li>In a grid search, various combinations of hyperparameters are tried and <span style=\"color:blue\">k-fold cross validation</span> is used to test the efficacy of the parameter combination</li>\n",
    "<li>the best combination is then selected as a candidate model</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">The <span style=\"color:blue\">scoring</span> parameter</h3>\n",
    "<li>since our data is imbalaced, we should look for the model with the best f1 score (precision/recall tradeoff)</li>\n",
    "<li>set the scoring parameter for GridSearchCV so that it maximizes the f1 score</li>\n",
    "<li>Though we should be using a much wider range of parameters, I've reduced them so that it runs fairly quickly</li>\n",
    "<li>This takes about 30 seconds on my machine. Could take longer on your machine</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Set up the hyperparameter options in param_grid\n",
    "param_grid = \n",
    "\n",
    "#Do the search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Get the best model parameters</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Run the best model and report metrics</h3>\n",
    "<li>Run the classifier using the best parameters</li>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix: \\n\",cfm)\n",
    "print(\"Training accuracy: \",accuracy_training)\n",
    "print(\"Testing  accuracy: \",accuracy_testing)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1-Score: \",f1)\n",
    "print(\"AUC: \",auc)\n",
    "print(\"Average Precision: \",ap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpret the results</h3>\n",
    "\n",
    "\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results_df</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Random Forest Classifier</h3>\n",
    "<li>We need to improve recall and precision so perhaps a non-linear classifier will help</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Build, fit, and report metrics</h3>\n",
    "\n",
    "<li>Run this with the following parameters (these are our base parameters)</li>\n",
    "<li>random_state=42,n_estimators=30,max_depth=6,min_samples_leaf=2000,min_samples_split=4000,class_weight={1:5}</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_4 = RandomForestClassifier(random_state=42,n_estimators=30,max_depth=6,min_samples_leaf=500,min_samples_split=4000,class_weight={1:5})\n",
    "model_4.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Confusion Matrix: \\n\",cfm)\n",
    "print(\"Training accuracy: \",accuracy_training)\n",
    "print(\"Testing  accuracy: \",accuracy_testing)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1-Score: \",f1)\n",
    "print(\"AUC: \",auc)\n",
    "print(\"Average Precision: \",ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpreting model 4 results</h3>\n",
    "<p></p>\n",
    "\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results_df</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 5</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Random Forest Grid Search</h3>\n",
    "<p></p>\n",
    "\n",
    "\n",
    "<li>Run the best model</li>\n",
    "<li>Note that this will take a while, perhaps even a couple of hours (25 minutes on my laptop). Let it run. Get some coffee or whatever beverage you like. Then come back in a while to check out the results!</li>\n",
    "<li>If you want to speed it up, remove the 500 option from n_estimators (n_estimators is the number of trees generated and is the single most expensive part of the grid search)</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import average_precision_score,make_scorer\n",
    "parameters = {\n",
    "     'n_estimators':(500,800), #the number of trees\n",
    "     'min_samples_split': (100, 200),\n",
    "    'class_weight': [{1:4},{1:6}],\n",
    "     'min_samples_leaf': (10,20) #\n",
    "}\n",
    "gs_clf = GridSearchCV(RandomForestClassifier(random_state=42),parameters,cv=5,n_jobs=-1,\n",
    "                      scoring='f1')\n",
    "gs_clf.fit(x_train, np.ravel(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Get the best model parameters</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Run the best model and get metrics</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training accuracy: \",accuracy_training)\n",
    "print(\"Testing  accuracy: \",accuracy_testing)\n",
    "print(\"confusion matrix:\")\n",
    "print(cfm)\n",
    "\n",
    "print(\"precision: \",precision)\n",
    "print(\"recall: \",recall)\n",
    "print(\"f1 score: \",f1)\n",
    "print(\"auc\",auc)\n",
    "print(\"ap\",ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpreting model 5 results</h3>\n",
    "\n",
    "<p>\n",
    "    </p>\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results df</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 6</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Gradient Boosting Classifier</li>\n",
    "<li>Grid search on GBC can take several days so let's just skip to the best models (I ran a 2-day reduced version)!</li>\n",
    "<li>Sklearn's gradient boosting classifier uses a sample weight vector to correct for imbalances in the data</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#sample_weight is a vector that indicates the weight of each \n",
    "#case in the training sample\n",
    "#If you're interested, try values from 1 to 10 instead of 4\n",
    "sample_weight = np.array([4 if i == 1 else 1 for i in y_train])\n",
    "\n",
    "\n",
    "model_6 = GradientBoostingClassifier(min_samples_split=100,\n",
    "                                     max_depth=8,\n",
    "                                 min_samples_leaf=100,\n",
    "                                 n_estimators=400,\n",
    "                                 subsample=0.6)\n",
    "\n",
    "\n",
    "model_6.fit(x_train,y_train,sample_weight=sample_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and print metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpreting model 6 results</h3>\n",
    "\n",
    "<p>\n",
    "    </p>\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results df</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:red;font-size:xx-large\">Build Model 7</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Same parameters but up the sample weight to 5</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#sample_weight is a vector that indicates the weight of each \n",
    "#case in the training sample\n",
    "#If you're interested, try values from 1 to 10 instead of 4\n",
    "sample_weight = np.array([5 if i == 1 else 1 for i in y_train])\n",
    "\n",
    "\n",
    "model_7 = GradientBoostingClassifier(min_samples_split=100,\n",
    "                                     max_depth=8,\n",
    "                                 min_samples_leaf=100,\n",
    "                                 n_estimators=400,\n",
    "                                 subsample=0.6)\n",
    "model_7.fit(x_train,y_train,sample_weight=sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate and print metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpreting model 7 results</h3>\n",
    "\n",
    "<p>\n",
    "    </p>\n",
    "<h4>Interpretation</h4>\n",
    "<li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Update results df</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-size:xx-large\">Model comparison</h3>\n",
    "<li>Draw a graph that shows the changes to accuracy, precision, recall, and f1 score</li>\n",
    "<li>The x-axis contains the five models you have created</li>\n",
    "<li>Use bokeh for the charts</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource, LabelSet, HoverTool\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHART \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpret the chart</h3>\n",
    "<li>What can you say about the changes in precision and recall</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Chart AUC and AP</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHART\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Interpret the AUC/AP chart</h3>\n",
    "<li>The AUC on the first 4 models is pretty much the same. What does that mean?</li>\n",
    "<ul><li></li>\n",
    "    </ul>\n",
    "<li>The average precision improves steadily but almost entirely by getting better at recall than at precision. What does that mean?</li>\n",
    "    \n",
    "<li></li>\n",
    "\n",
    "<li>Finally, what can you do to get better results? </li>\n",
    "<ul>\n",
    "    <li></li></ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
